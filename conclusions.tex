\section{Conclusions} \label{sec:conclusions}

In this paper, we proposed a system for automatic music transcription which incorporated prior information from a polyphonic music prediction model based on recurrent neural networks. The acoustic transcription model was based on probabilistic latent component analysis, and information from the prediction system was incorporated using Dirichlet priors. Experimental results using the Bach10 dataset of multiple-instrument recordings showed that there is a clear and significant improvement (3\% in terms of F-measure) by combining a music language model with an acoustic model for improving the performance of the latter.

In the current evaluation, the language models are trained on only one dataset. In the future, we would like to evaluate the proposed system using language models trained from different sources to see if this helps the MLMs generalize better. We will also investigate different system configurations, by bootstrapping the system for demonstrating that an improved transcription can lead to an improved prediction, and so on. We will also investigate the effect of using different RNN architectures like Long Short Term Memory (LSTM) and bi-directional RNNs and LSTMs. Finally, we would like to extend and improve the current models for high-dimensional sequences to better fit the requirements for music language modelling. 