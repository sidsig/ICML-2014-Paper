\section{Introduction} 
\label{sec:introduction}

Automatic Music Transcription (AMT) involves automatically generating a symbolic representation of an acoustic musical signal \cite{Benetos2013b}. AMT has is considered to be a fundamental topic in the field of music information retrieval (MIR) and has noumerous applications in related fields in music technology, such as interactive music applications and computational musicology. Typically, the output of an AMT systemis a \textit{pianoroll} representation, which is a two-dimensional matrix representation of a musical piece where the X-axis represents time quantized into regular intervals, and the Y-axis represents the $88$ keys of a piano in increasing pitch. A cell in this matrix is $1$ if the key represented by its X-coordinate is sounded at the time instant represented by its Y-coordinate.

The majority of recent transcription papers utilise and expand \emph{spectrogram factorisation} techniques, such as non-negative matrix factorisation (NMF) \cite{Li1999} and its probabilistic counterpart, probabilistic latent component analysis (PLCA) \cite{Smaragdis2006}. Spectrogram factorisation techniques decompose an input two-dimensional spectrogram of the audio signal into a product of spectral templates (that typically correspond to musical notes) and component activations (that indicate when each note is active at a given time frame). Spectrogram factorisation-based AMT systems include the work by Bertin et al.\ \cite{Bertin2009}, who proposed a Bayesian framework for NMF, which considers each pitch as a model of Gaussian components in harmonic positions. Benetos and Dixon \cite{Benetos2012} proposed a convolutive model based on PLCA, which supports the transcription of multiple-instrument music and supports tuning changes and frequency modulations (modelled as shifts across log-frequency). An alternative approach for AMT was proposed in \cite{Nam2011}, where features suitable for transcribing music are learned using a deep belief network consisting of stacked restricted Boltzmann machines (RBMs). The model performed classification using support vector machines and was applied to piano music.

There is no doubt that a reliable acoustic model is important for generating accurate symbolic transcriptions of a given music signal. However, since music exhibits a fair amount of structural regularity much like language, it is natural for one to think of the possibility of improving transcription accuracy using a \textit{music language model} (MLM) in a manner akin to the use of a language model to improve the performance of a speech recognizer \cite{Rabiner1993}. In \cite{Boulanger-Lewandowski2012}, the predictions of a polyphonic MLM were used to this end. More generally, \textit{score informed} approaches have been found to benefit the performance of purely acoustic models in music research tasks such as source separation \cite{Ewert2012}, voice separation \cite{Ewert2011} and tonic identification \cite{Senturk2013}. % More references, if required: Source separation - Ganseman2010, Hennequin2011

In the present work, we make use of the predictions made by a Recurrent Neural Network-Neural Autoregressive Distribution Estimator (RNN-NADE) based polyphonic MLM proposed in \cite{Boulanger-Lewandowski2012} to refine the transcriptions of a PLCA based AMT system \cite{Benetos2012, Benetos2013}. \textit{NOTE: Summary of the combination strategy using Dirichlet priors, etc. could go here}. It was observed that combining the two models in this way boosts transcription accuracy to $100.00\%$ on the Bach-$10$ dataset, where the existing state-of-the-art accuracy is $99.00\%$.

The outline of this paper is as follows. The PLCA-based transcription system is presented in Section \ref{sec:transcription}. The RNN-RBM-based polyphonic music prediction system that is used as a music language model is described in Section \ref{sec:prediction}. The combination of the two aforementioned systems is presented in Section \ref{sec:combination}. The employed dataset, evaluation metrics, and experimental results are shown in Section \ref{sec:evaluation}; finally, conclusions are drawn and future directions are indicated in Section \ref{sec:conclusions}.